{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(df: pd.DataFrame, cont_vars: list, target: str):\n",
    "    \"\"\"\n",
    "    Encodes categorical features, extracts predictors, and splits the data into training and imputation sets.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    cont_vars (list): List of continuous variables.\n",
    "    target (str): Target variable to predict.\n",
    "\n",
    "    Returns:\n",
    "    tuple: (df, predictors, X_train_split, X_val, y_train_split, y_val, X_impute_data)\n",
    "    \"\"\"\n",
    "    # One-hot encode categorical variables\n",
    "    df_encoded = pd.get_dummies(df, columns=['state_name'], drop_first=True)\n",
    "\n",
    "    # Get predictor columns\n",
    "    predictors = cont_vars + [col for col in df_encoded.columns if col.startswith('state_name_')]\n",
    "\n",
    "    # Split data into training and imputation sets\n",
    "    train_data = df_encoded[df_encoded[target].notnull()]\n",
    "    X_train = train_data[predictors]\n",
    "    y_train = train_data[target]\n",
    "\n",
    "    impute_data = df_encoded[df_encoded[target].isnull()]\n",
    "    X_impute_data = impute_data[predictors]\n",
    "\n",
    "    X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    return df, predictors, X_train_split, X_val, y_train_split, y_val, X_impute_data\n",
    "\n",
    "def train_and_impute(df: pd.DataFrame, predictors: list, X_train_split, X_val, y_train_split, y_val, X_impute_data, target: str):\n",
    "    \"\"\"\n",
    "    Trains a RandomForestRegressor, evaluates performance, and imputes missing target values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The original DataFrame.\n",
    "    predictors (list): List of predictor variables.\n",
    "    X_train_split (pd.DataFrame): Training predictors.\n",
    "    X_val (pd.DataFrame): Validation predictors.\n",
    "    y_train_split (pd.Series): Training target.\n",
    "    y_val (pd.Series): Validation target.\n",
    "    X_impute_data (pd.DataFrame): Predictors for missing target values.\n",
    "    target (str): The target variable to predict.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    rf_model.fit(X_train_split, y_train_split)\n",
    "\n",
    "    # Evaluate model performance\n",
    "    y_val_pred_rf = rf_model.predict(X_val)\n",
    "    print(f\"Random Forest R² Score: {r2_score(y_val, y_val_pred_rf):.4f}\")\n",
    "\n",
    "    # Impute missing values\n",
    "    df.loc[df[target].isnull(), target] = rf_model.predict(X_impute_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "def execute_pipeline(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Executes the full pipeline: preprocessing, training, and imputing missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The final DataFrame with missing target values imputed.\n",
    "    \"\"\"\n",
    "    cont_vars = [\n",
    "        'median_household_income', 'population',\n",
    "        'median_age', 'intersection_density', 'population_density',\n",
    "        'pct_bachelor'\n",
    "    ]\n",
    "    target = 'median_sale_price'\n",
    "\n",
    "    # Preprocess data\n",
    "    df, predictors, X_train_split, X_val, y_train_split, y_val, X_impute_data = preprocess_data(df, cont_vars, target)\n",
    "\n",
    "    # Train model and impute missing values\n",
    "    df_final = train_and_impute(df, predictors, X_train_split, X_val, y_train_split, y_val, X_impute_data, target)\n",
    "    \n",
    "    return df_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_composite_score(df, cont_vars, adjustments, scale_range=(0, 100)):\n",
    "    \"\"\"\n",
    "    Computes a composite score from continuous variables using PCA.\n",
    "\n",
    "    This function:\n",
    "    1. Adjusts continuous variables based on provided adjustment factors.\n",
    "    2. Standardizes the adjusted values using `StandardScaler()`.\n",
    "    3. Performs Principal Component Analysis (PCA) and extracts the first principal component.\n",
    "    4. Scales the principal component to a specified range (default: 0-100).\n",
    "    5. Returns the modified DataFrame with the composite score and PCA loadings.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing continuous variables.\n",
    "    cont_vars (list): List of continuous variable column names to be used in PCA.\n",
    "    adjustments (dict): Dictionary where keys are variable names and values are adjustment factors.\n",
    "                        If a variable is not in the dictionary, it defaults to a factor of 1.\n",
    "    scale_range (tuple, optional): The target range for the composite score scaling. Default is (0, 100).\n",
    "\n",
    "    Returns:\n",
    "    tuple:\n",
    "        - pd.DataFrame: A copy of the input DataFrame with an added 'composite_score' column.\n",
    "        - np.ndarray: The PCA loadings (importance of each variable in the principal component).\n",
    "    \"\"\"\n",
    "    # Create a copy of selected continuous variables and apply adjustments\n",
    "    X = df[cont_vars].copy()\n",
    "    for var in cont_vars:\n",
    "        factor = adjustments.get(var, 1)  # Default adjustment factor is 1\n",
    "        X[var] = X[var] * factor\n",
    "    \n",
    "    X_array = X.values\n",
    "\n",
    "    # Standardize the adjusted data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_array)\n",
    "\n",
    "    # Perform PCA to reduce dimensionality to one component\n",
    "    pca = PCA(n_components=1)\n",
    "    pc1 = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Scale the principal component to the specified range\n",
    "    min_val, max_val = np.min(pc1), np.max(pc1)\n",
    "    if max_val == min_val:\n",
    "        scaled_pc1 = np.full_like(pc1, scale_range[0])  # Avoid division by zero\n",
    "    else:\n",
    "        scaled_pc1 = (pc1 - min_val) / (max_val - min_val) * (scale_range[1] - scale_range[0]) + scale_range[0]\n",
    "\n",
    "    # Add the composite score to the DataFrame\n",
    "    df_out = df.copy()\n",
    "    df_out['composite_score'] = scaled_pc1.flatten()\n",
    "\n",
    "    # Extract PCA loadings (importance of each variable in the first principal component)\n",
    "    loadings = pca.components_[0]\n",
    "    \n",
    "    return df_out, loadings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the full data processing pipeline.\n",
    "\n",
    "    Steps:\n",
    "    1. Add the current directory to the system path.\n",
    "    2. Define input and output file paths.\n",
    "    3. Load Redfin and ACS datasets.\n",
    "    4. Merge ACS data with Redfin data.\n",
    "    5. Drop duplicates and process the data pipeline.\n",
    "    6. Compute the composite score.\n",
    "    7. Round specific columns for better readability.\n",
    "    8. Save the final processed dataset to a CSV file.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the script can access necessary modules\n",
    "    sys.path.append(os.path.abspath('.'))\n",
    "    print(\"Added current directory to system path.\")\n",
    "\n",
    "    # Define file paths\n",
    "    redfin_path = os.path.join('..', 'data', 'redfin_clean.csv')\n",
    "    acs_path = os.path.join('..', 'data', 'acs_with_county.csv')\n",
    "    output_path = os.path.join('..', 'data', 'final_data.csv')\n",
    "\n",
    "    print(f\"Loading data from:\\n  - {redfin_path}\\n  - {acs_path}\")\n",
    "\n",
    "    # Load datasets\n",
    "    df_redfin = pd.read_csv(redfin_path)\n",
    "    df_acs_with_county = pd.read_csv(acs_path)\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "\n",
    "    # Merge datasets\n",
    "    print(\"Merging ACS and Redfin data...\")\n",
    "    df_merged = df_acs_with_county.merge(df_redfin, on='county', how='left')\n",
    "\n",
    "    # Drop duplicates, keeping the last occurrence\n",
    "    df_final = df_merged.drop_duplicates(subset=['state_name', 'town'], keep='last')\n",
    "    print(\"Duplicates removed.\")\n",
    "\n",
    "    # Execute the data processing pipeline\n",
    "    print(\"Executing data pipeline...\")\n",
    "    df_final = execute_pipeline(df_final)\n",
    "    print(\"Pipeline execution completed.\")\n",
    "\n",
    "    # Compute composite score\n",
    "    print(\"Computing composite score...\")\n",
    "    df_out, loadings = compute_composite_score(\n",
    "        df=df_final,\n",
    "        cont_vars=['median_household_income', 'median_age', 'intersection_density',\n",
    "                   'pct_bachelor', 'median_sale_price'],\n",
    "        adjustments={\n",
    "            'age': -1,\n",
    "            'median_sale_price': 0.75\n",
    "        }\n",
    "    )\n",
    "    print(\"Composite score computed.\")\n",
    "\n",
    "    # Round selected columns\n",
    "    cols_to_round = ['intersection_density', 'population_density',\n",
    "                     'pct_bachelor', 'median_sale_price', 'composite_score']\n",
    "    df_out[cols_to_round] = df_out[cols_to_round].round(2)\n",
    "\n",
    "    # Save final processed data\n",
    "    print(f\"Saving final dataset to {output_path}...\")\n",
    "    df_out.to_csv(output_path, index=False)\n",
    "    print(\"Final dataset saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added current directory to system path.\n",
      "Loading data from:\n",
      "  - ../data/redfin_clean.csv\n",
      "  - ../data/acs_with_county.csv\n",
      "Datasets loaded successfully.\n",
      "Merging ACS and Redfin data...\n",
      "Duplicates removed.\n",
      "Executing data pipeline...\n",
      "Random Forest R² Score: 0.6255\n",
      "Pipeline execution completed.\n",
      "Computing composite score...\n",
      "Composite score computed.\n",
      "Saving final dataset to ../data/final_data.csv...\n",
      "Final dataset saved successfully.\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
